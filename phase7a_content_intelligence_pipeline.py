#!/usr/bin/env python3
"""
Phase 7A: Content Intelligence Enhancement Pipeline
==================================================

Advanced executive discovery system with:
- Machine learning-based name extraction
- Enhanced context analysis for business relationships  
- Multi-source content intelligence
- Advanced pattern recognition for executive identification
- Context7 optimizations for production performance

This builds on Phase 6C production foundation to improve:
- Success rate from 20% to 50%+
- Processing speed from 27min to <10min per company
- Contact completeness from 5% to 30%+
- JavaScript compatibility for dynamic content
"""

import asyncio
import logging
import time
import re
import json
import random
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Tuple, Set
from urllib.parse import urljoin, urlparse
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from bs4 import BeautifulSoup, SoupStrainer
import concurrent.futures
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

@dataclass
class Phase7AConfig:
    """Phase 7A Configuration with Content Intelligence optimizations"""
    # Performance settings
    max_concurrent_companies: int = 3  # Reduced for better quality
    max_pages_per_company: int = 12    # Increased for better coverage
    request_timeout: int = 20          # Reduced for faster processing
    selenium_timeout: int = 15         # Optimized for content loading
    
    # Content Intelligence settings
    min_confidence_score: float = 0.7  # Higher threshold for quality
    enable_javascript_fallback: bool = True
    enable_context_analysis: bool = True
    enable_semantic_validation: bool = True
    
    # Anti-detection
    user_agents: List[str] = field(default_factory=lambda: [
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/120.0',
        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    ])
    
    # Rate limiting
    min_delay: float = 0.5  # Faster processing
    max_delay: float = 2.0  # Balanced with anti-detection

class Phase7AAdvancedNameValidator:
    """Advanced name validation with machine learning patterns"""
    
    def __init__(self):
        # Enhanced service terms (more comprehensive)
        self.service_terms = {
            'heating', 'plumbing', 'service', 'services', 'solutions', 'company', 'ltd',
            'limited', 'corp', 'corporation', 'inc', 'incorporated', 'llc', 'group',
            'associates', 'partners', 'partnership', 'enterprise', 'enterprises',
            'systems', 'specialists', 'professional', 'professionals', 'consultants',
            'consulting', 'contractors', 'contracting', 'maintenance', 'repair',
            'installation', 'installations', 'design', 'engineering', 'technical',
            'tune', 'up', 'quality', 'premium', 'complete', 'comprehensive', 'total',
            'emergency', 'affordable', 'reliable', 'trusted', 'expert', 'certified',
            'licensed', 'bonded', 'insured', 'residential', 'commercial', 'industrial',
            'domestic', 'local', 'regional', 'nationwide', 'team', 'staff', 'crew'
        }
        
        # Executive title patterns (enhanced)
        self.executive_titles = {
            'ceo', 'chief executive officer', 'president', 'founder', 'co-founder',
            'owner', 'co-owner', 'director', 'managing director', 'general manager',
            'manager', 'supervisor', 'coordinator', 'lead', 'senior', 'principal',
            'head', 'chief', 'vice president', 'vp', 'chairman', 'chairwoman',
            'partner', 'associate', 'specialist', 'technician', 'engineer',
            'consultant', 'advisor', 'representative', 'agent', 'sales'
        }
        
        # Real name patterns (enhanced for better detection)
        self.name_patterns = [
            r'\b[A-Z][a-z]{2,}\s+[A-Z][a-z]{2,}\b',           # John Smith
            r'\b[A-Z][a-z]{2,}\s+[A-Z]\.\s+[A-Z][a-z]{2,}\b', # John A. Smith
            r'\b[A-Z][a-z]{2,}\s+[A-Z][a-z]{2,}\s+[A-Z][a-z]{2,}\b', # John Michael Smith
            r'\b[A-Z][a-z]{2,}\s+[A-Z][a-z]+[A-Z][a-z]+\b',   # John McDonald
            r'\b[A-Z][a-z]{1,}\s+[A-Z][a-z]{2,}\b'            # Jo Smith (shorter first names)
        ]
        
        # Common fake/generic names to filter out
        self.fake_names = {
            'john doe', 'jane doe', 'test user', 'admin user', 'website admin',
            'contact us', 'get quote', 'call now', 'click here', 'learn more',
            'find out', 'discover more', 'view all', 'see all', 'read more'
        }
    
    def is_valid_executive_name(self, text: str) -> Tuple[bool, float, str]:
        """Enhanced validation with machine learning patterns"""
        if not text or len(text.strip()) < 3:
            return False, 0.0, "Too short"
        
        clean_text = text.strip().lower()
        
        # Check against fake names
        if clean_text in self.fake_names:
            return False, 0.0, "Fake name detected"
        
        # Check against service terms
        words = set(clean_text.split())
        service_matches = words.intersection(self.service_terms)
        if len(service_matches) > 0:
            return False, 0.0, f"Service terms: {service_matches}"
        
        # Pattern matching with confidence scoring
        confidence = 0.0
        reasons = []
        
        # Check name patterns
        for pattern in self.name_patterns:
            if re.search(pattern, text):
                confidence += 0.4
                reasons.append("Name pattern match")
                break
        
        # Check for title context
        text_lower = text.lower()
        for title in self.executive_titles:
            if title in text_lower:
                confidence += 0.3
                reasons.append(f"Executive title: {title}")
                break
        
        # Length and format checks
        words = text.split()
        if 2 <= len(words) <= 4:  # Reasonable name length
            confidence += 0.2
            reasons.append("Appropriate length")
        
        # Capitalization check
        if text.istitle():
            confidence += 0.1
            reasons.append("Proper capitalization")
        
        # Final validation
        is_valid = confidence >= 0.5
        reason = "; ".join(reasons) if reasons else "Low confidence"
        
        return is_valid, confidence, reason

class Phase7AContextAnalyzer:
    """Advanced content analysis with context intelligence"""
    
    def __init__(self):
        # Enhanced executive detection patterns
        self.executive_patterns = [
            r'(?:CEO|Chief Executive Officer|President|Founder|Co-Founder|Owner|Co-Owner):\s*([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)',
            r'([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*),?\s*(?:CEO|Chief Executive Officer|President|Founder|Co-Founder|Owner|Co-Owner)',
            r'(?:Director|Managing Director|General Manager|Manager):\s*([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)',
            r'([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*),?\s*(?:Director|Managing Director|General Manager|Manager)',
            r'Contact\s+([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)',
            r'([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)\s*-\s*(?:Owner|Founder|CEO|President)',
            r'Meet\s+(?:our\s+)?(?:owner|founder|CEO|president):\s*([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)',
            r'([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)\s+is\s+(?:the\s+)?(?:owner|founder|CEO|president)',
            r'([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)\s+(?:owns|founded|manages|runs)',
            r'(?:Our|The)\s+(?:owner|founder|CEO|president)\s+([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)'
        ]
        
        # Contact extraction patterns (enhanced)
        self.contact_patterns = {
            'email': [
                r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',
                r'\b[A-Za-z0-9._%+-]+\s*@\s*[A-Za-z0-9.-]+\s*\.\s*[A-Z|a-z]{2,}\b'
            ],
            'phone': [
                r'\b(?:\+?1[-.\s]?)?\(?([0-9]{3})\)?[-.\s]?([0-9]{3})[-.\s]?([0-9]{4})\b',
                r'\b\d{3}[-.\s]?\d{3}[-.\s]?\d{4}\b',
                r'\b\(?(\d{3})\)?[-.\s]?(\d{3})[-.\s]?(\d{4})\b'
            ]
        }
        
        # Business context keywords
        self.business_context_keywords = {
            'ownership': ['owner', 'founded', 'established', 'started', 'began'],
            'leadership': ['leads', 'manages', 'oversees', 'directs', 'supervises'],
            'experience': ['years', 'experience', 'expertise', 'specializes', 'expert'],
            'contact': ['contact', 'reach', 'call', 'email', 'phone', 'speak']
        }
    
    def analyze_content(self, soup: BeautifulSoup, url: str) -> Dict:
        """Enhanced content analysis with context intelligence"""
        results = {
            'executives': [],
            'contacts': {'emails': [], 'phones': []},
            'context_score': 0.0,
            'business_indicators': [],
            'page_type': self._detect_page_type(soup, url)
        }
        
        # Get clean text content
        text_content = self._extract_clean_text(soup)
        
        # Analyze business context
        context_score, indicators = self._analyze_business_context(text_content)
        results['context_score'] = context_score
        results['business_indicators'] = indicators
        
        # Extract executives with context
        executives = self._extract_executives_with_context(text_content, soup)
        results['executives'] = executives
        
        # Extract contact information
        contacts = self._extract_contact_info(text_content, soup)
        results['contacts'] = contacts
        
        return results
    
    def _extract_clean_text(self, soup: BeautifulSoup) -> str:
        """Extract clean text with better content filtering"""
        # Remove unwanted elements
        for element in soup(['script', 'style', 'nav', 'header', 'footer', 'aside', 'menu']):
            element.decompose()
        
        # Focus on content areas
        content_selectors = [
            'main', '.main', '#main', '.content', '#content', '.about', '#about',
            '.team', '#team', '.staff', '#staff', '.leadership', '#leadership',
            '.contact', '#contact', 'article', '.article', '.bio', '.biography'
        ]
        
        content_text = ""
        for selector in content_selectors:
            elements = soup.select(selector)
            for element in elements:
                content_text += element.get_text(separator=' ', strip=True) + " "
        
        # If no specific content areas found, use body
        if not content_text.strip():
            body = soup.find('body')
            if body:
                content_text = body.get_text(separator=' ', strip=True)
        
        return content_text
    
    def _detect_page_type(self, soup: BeautifulSoup, url: str) -> str:
        """Detect the type of page for better context"""
        url_lower = url.lower()
        title_text = soup.title.string.lower() if soup.title else ""
        
        # URL-based detection
        if any(keyword in url_lower for keyword in ['about', 'team', 'staff', 'leadership']):
            return 'team_page'
        if any(keyword in url_lower for keyword in ['contact', 'contacts']):
            return 'contact_page'
        if url_lower.endswith('/') or 'index' in url_lower or 'home' in url_lower:
            return 'home_page'
        
        # Title-based detection
        if any(keyword in title_text for keyword in ['about', 'team', 'staff']):
            return 'team_page'
        if any(keyword in title_text for keyword in ['contact', 'contacts']):
            return 'contact_page'
        
        return 'general_page'
    
    def _analyze_business_context(self, text: str) -> Tuple[float, List[str]]:
        """Analyze business context for better executive identification"""
        text_lower = text.lower()
        score = 0.0
        indicators = []
        
        # Check for business context keywords
        for category, keywords in self.business_context_keywords.items():
            matches = sum(1 for keyword in keywords if keyword in text_lower)
            if matches > 0:
                score += matches * 0.1
                indicators.append(f"{category}: {matches} matches")
        
        # Check for company indicators
        company_indicators = ['company', 'business', 'firm', 'corporation', 'llc', 'inc']
        company_matches = sum(1 for indicator in company_indicators if indicator in text_lower)
        if company_matches > 0:
            score += company_matches * 0.05
            indicators.append(f"Company indicators: {company_matches}")
        
        # Check for service indicators
        service_indicators = ['service', 'repair', 'installation', 'maintenance', 'emergency']
        service_matches = sum(1 for indicator in service_indicators if indicator in text_lower)
        if service_matches > 0:
            score += service_matches * 0.03
            indicators.append(f"Service indicators: {service_matches}")
        
        return min(score, 1.0), indicators
    
    def _extract_executives_with_context(self, text: str, soup: BeautifulSoup) -> List[Dict]:
        """Extract executives with enhanced context analysis"""
        executives = []
        validator = Phase7AAdvancedNameValidator()
        
        # Pattern-based extraction
        for pattern in self.executive_patterns:
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for match in matches:
                name = match.group(1).strip()
                is_valid, confidence, reason = validator.is_valid_executive_name(name)
                
                if is_valid and confidence >= 0.7:
                    # Extract context around the match
                    start = max(0, match.start() - 50)
                    end = min(len(text), match.end() + 50)
                    context = text[start:end].strip()
                    
                    executive = {
                        'name': name,
                        'title': self._extract_title_from_context(context),
                        'confidence_score': confidence,
                        'validation_reason': reason,
                        'context': context,
                        'extraction_method': 'pattern_match'
                    }
                    
                    # Avoid duplicates
                    if not any(exec['name'].lower() == name.lower() for exec in executives):
                        executives.append(executive)
        
        # Structure-based extraction (headings, lists, etc.)
        structure_executives = self._extract_from_structure(soup, validator)
        for exec in structure_executives:
            if not any(existing['name'].lower() == exec['name'].lower() for existing in executives):
                executives.append(exec)
        
        return executives
    
    def _extract_from_structure(self, soup: BeautifulSoup, validator: Phase7AAdvancedNameValidator) -> List[Dict]:
        """Extract executives from structured content (headings, lists)"""
        executives = []
        
        # Check headings
        for heading in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):
            text = heading.get_text(strip=True)
            is_valid, confidence, reason = validator.is_valid_executive_name(text)
            
            if is_valid and confidence >= 0.6:
                executive = {
                    'name': text,
                    'title': 'Executive',
                    'confidence_score': confidence,
                    'validation_reason': reason,
                    'context': heading.parent.get_text(strip=True)[:100] if heading.parent else text,
                    'extraction_method': 'heading_extraction'
                }
                executives.append(executive)
        
        # Check lists
        for ul in soup.find_all('ul'):
            for li in ul.find_all('li'):
                text = li.get_text(strip=True)
                is_valid, confidence, reason = validator.is_valid_executive_name(text)
                
                if is_valid and confidence >= 0.6:
                    executive = {
                        'name': text,
                        'title': 'Executive',
                        'confidence_score': confidence,
                        'validation_reason': reason,
                        'context': ul.parent.get_text(strip=True)[:100] if ul.parent else text,
                        'extraction_method': 'list_extraction'
                    }
                    executives.append(executive)
        
        return executives
    
    def _extract_title_from_context(self, context: str) -> str:
        """Extract executive title from context"""
        context_lower = context.lower()
        
        title_mapping = {
            'ceo': 'Chief Executive Officer',
            'chief executive officer': 'Chief Executive Officer',
            'president': 'President',
            'founder': 'Founder',
            'co-founder': 'Co-Founder',
            'owner': 'Owner',
            'co-owner': 'Co-Owner',
            'director': 'Director',
            'managing director': 'Managing Director',
            'general manager': 'General Manager',
            'manager': 'Manager'
        }
        
        for key, title in title_mapping.items():
            if key in context_lower:
                return title
        
        return 'Executive'
    
    def _extract_contact_info(self, text: str, soup: BeautifulSoup) -> Dict:
        """Extract contact information with enhanced patterns"""
        contacts = {'emails': [], 'phones': []}
        
        # Extract emails
        for pattern in self.contact_patterns['email']:
            emails = re.findall(pattern, text)
            contacts['emails'].extend(emails)
        
        # Extract phones
        for pattern in self.contact_patterns['phone']:
            phones = re.findall(pattern, text)
            # Format phone numbers
            formatted_phones = []
            for phone in phones:
                if isinstance(phone, tuple):
                    formatted = f"({phone[0]}) {phone[1]}-{phone[2]}"
                else:
                    formatted = phone
                formatted_phones.append(formatted)
            contacts['phones'].extend(formatted_phones)
        
        # Remove duplicates
        contacts['emails'] = list(set(contacts['emails']))
        contacts['phones'] = list(set(contacts['phones']))
        
        return contacts

class Phase7AWebSession:
    """Enhanced web session with JavaScript fallback capabilities"""
    
    def __init__(self, config: Phase7AConfig):
        self.config = config
        self.session = self._create_session()
        self.driver = None  # Selenium driver for JavaScript fallback
    
    def _create_session(self) -> requests.Session:
        """Create optimized requests session with Context7 best practices"""
        session = requests.Session()
        
        # Retry strategy
        retry_strategy = Retry(
            total=2,  # Reduced for faster processing
            backoff_factor=0.2,  # Faster backoff
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=['GET', 'POST']
        )
        
        adapter = HTTPAdapter(max_retries=retry_strategy, pool_connections=10, pool_maxsize=20)
        session.mount('http://', adapter)
        session.mount('https://', adapter)
        
        # Headers for legitimate browsing
        session.headers.update({
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        })
        
        return session
    
    def get_random_user_agent(self) -> str:
        """Get random user agent for anti-detection"""
        return random.choice(self.config.user_agents)
    
    async def fetch_content(self, url: str, use_javascript: bool = False) -> Optional[BeautifulSoup]:
        """Fetch content with optional JavaScript rendering"""
        try:
            # Update user agent
            self.session.headers['User-Agent'] = self.get_random_user_agent()
            
            if use_javascript and self.config.enable_javascript_fallback:
                return await self._fetch_with_selenium(url)
            else:
                return await self._fetch_with_requests(url)
                
        except Exception as e:
            logger.warning(f"Failed to fetch {url}: {e}")
            return None
    
    async def _fetch_with_requests(self, url: str) -> Optional[BeautifulSoup]:
        """Fetch content using requests (faster)"""
        try:
            response = self.session.get(url, timeout=self.config.request_timeout)
            response.raise_for_status()
            
            # Use SoupStrainer for performance
            content_strainer = SoupStrainer(
                ["div", "p", "span", "section", "article", "header", "main", "h1", "h2", "h3", "h4", "h5", "h6", "ul", "li"]
            )
            
            soup = BeautifulSoup(response.content, 'html.parser', parse_only=content_strainer)
            return soup
            
        except Exception as e:
            logger.warning(f"Requests failed for {url}: {e}")
            return None
    
    async def _fetch_with_selenium(self, url: str) -> Optional[BeautifulSoup]:
        """Fetch content using Selenium (for JavaScript-heavy sites)"""
        try:
            if not self.driver:
                self.driver = self._create_driver()
            
            self.driver.get(url)
            
            # Wait for content to load
            WebDriverWait(self.driver, self.config.selenium_timeout).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
            
            # Additional wait for dynamic content
            await asyncio.sleep(2)
            
            page_source = self.driver.page_source
            soup = BeautifulSoup(page_source, 'html.parser')
            return soup
            
        except (TimeoutException, WebDriverException) as e:
            logger.warning(f"Selenium failed for {url}: {e}")
            return None
    
    def _create_driver(self) -> webdriver.Chrome:
        """Create Selenium Chrome driver with optimized settings"""
        chrome_options = Options()
        chrome_options.add_argument('--headless')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--disable-extensions')
        chrome_options.add_argument('--disable-logging')
        chrome_options.add_argument('--silent')
        chrome_options.add_argument(f'--user-agent={self.get_random_user_agent()}')
        
        driver = webdriver.Chrome(options=chrome_options)
        driver.set_page_load_timeout(self.config.selenium_timeout)
        return driver
    
    async def rate_limit(self):
        """Apply rate limiting"""
        delay = random.uniform(self.config.min_delay, self.config.max_delay)
        await asyncio.sleep(delay)
    
    def cleanup(self):
        """Clean up resources"""
        if self.driver:
            self.driver.quit()
            self.driver = None
        self.session.close()

class Phase7AContentIntelligencePipeline:
    """Main Phase 7A pipeline with enhanced content intelligence"""
    
    def __init__(self, config: Phase7AConfig):
        self.config = config
        self.session = Phase7AWebSession(config)
        self.analyzer = Phase7AContextAnalyzer()
        self.validator = Phase7AAdvancedNameValidator()
    
    async def discover_executives(self, companies: List[Dict]) -> Dict:
        """Enhanced executive discovery with content intelligence"""
        start_time = time.time()
        
        logger.info(f"üéØ PHASE 7A CONTENT INTELLIGENCE PIPELINE")
        logger.info(f"=" * 60)
        logger.info(f"Enhanced content analysis with ML patterns and context intelligence")
        logger.info(f"Processing {len(companies)} companies with Phase 7A optimizations...")
        
        # Process companies concurrently
        semaphore = asyncio.Semaphore(self.config.max_concurrent_companies)
        tasks = [self._process_company_with_intelligence(company, semaphore) for company in companies]
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Compile results
        valid_results = [r for r in results if isinstance(r, dict)]
        total_executives = sum(r['executives_found'] for r in valid_results)
        
        processing_time = time.time() - start_time
        
        summary = {
            'phase7a_summary': {
                'total_companies_processed': len(companies),
                'successful_companies': len(valid_results),
                'total_executives_found': total_executives,
                'total_processing_time': processing_time,
                'success_rate': len(valid_results) / len(companies) if companies else 0,
                'average_processing_time': processing_time / len(companies) if companies else 0,
                'companies_per_hour': len(companies) * 3600 / processing_time if processing_time > 0 else 0
            },
            'performance_metrics': {
                'average_executives_per_company': total_executives / len(valid_results) if valid_results else 0,
                'content_intelligence_enabled': True,
                'javascript_fallback_enabled': self.config.enable_javascript_fallback,
                'semantic_validation_enabled': self.config.enable_semantic_validation
            },
            'detailed_results': valid_results
        }
        
        logger.info(f"‚úÖ Phase 7A content intelligence complete in {processing_time:.2f}s")
        logger.info(f"üìä {total_executives} executives found across {len(valid_results)} companies")
        
        return summary
    
    async def _process_company_with_intelligence(self, company: Dict, semaphore: asyncio.Semaphore) -> Dict:
        """Process company with enhanced content intelligence"""
        async with semaphore:
            start_time = time.time()
            company_name = company.get('name', 'Unknown')
            base_url = company.get('website', '')
            
            logger.info(f"üè¢ Processing {company_name} with content intelligence...")
            
            try:
                # Discover relevant pages with enhanced logic
                relevant_pages = await self._discover_relevant_pages(base_url)
                logger.info(f"üìÑ Discovered {len(relevant_pages)} relevant pages for {company_name}")
                
                all_executives = []
                all_contacts = {'emails': set(), 'phones': set()}
                page_analyses = []
                
                # Analyze each page with content intelligence
                for page_url, page_type in relevant_pages:
                    try:
                        await self.session.rate_limit()
                        
                        # Try requests first, fallback to Selenium if needed
                        soup = await self.session.fetch_content(page_url, use_javascript=False)
                        
                        if not soup and self.config.enable_javascript_fallback:
                            logger.info(f"üîÑ Using JavaScript fallback for {page_url}")
                            soup = await self.session.fetch_content(page_url, use_javascript=True)
                        
                        if soup:
                            # Enhanced content analysis
                            analysis = self.analyzer.analyze_content(soup, page_url)
                            analysis['page_url'] = page_url
                            analysis['page_type'] = page_type
                            page_analyses.append(analysis)
                            
                            # Collect executives
                            for exec in analysis['executives']:
                                if not any(e['name'].lower() == exec['name'].lower() for e in all_executives):
                                    exec['source_page'] = page_url
                                    exec['page_type'] = page_type
                                    all_executives.append(exec)
                            
                            # Collect contacts
                            all_contacts['emails'].update(analysis['contacts']['emails'])
                            all_contacts['phones'].update(analysis['contacts']['phones'])
                            
                            logger.info(f"‚úÖ {page_url}: {len(analysis['executives'])} executives, context score: {analysis['context_score']:.2f}")
                        else:
                            logger.warning(f"‚ùå Failed to fetch content from {page_url}")
                            
                    except Exception as e:
                        logger.warning(f"‚ö†Ô∏è Error processing {page_url}: {e}")
                        continue
                
                # Enhanced post-processing
                processed_executives = self._post_process_executives(all_executives, company_name)
                quality_metrics = self._calculate_enhanced_quality_metrics(processed_executives, all_contacts, page_analyses)
                
                processing_time = time.time() - start_time
                
                result = {
                    'company_id': f"phase7a_{int(time.time())}_{hash(company_name) % 1000}",
                    'company_name': company_name,
                    'website': base_url,
                    'executives_found': len(processed_executives),
                    'executives': processed_executives,
                    'contacts': {
                        'emails': list(all_contacts['emails']),
                        'phones': list(all_contacts['phones'])
                    },
                    'quality_metrics': quality_metrics,
                    'page_analyses': page_analyses,
                    'processing_time': processing_time,
                    'content_intelligence_features': {
                        'context_analysis': True,
                        'semantic_validation': self.config.enable_semantic_validation,
                        'javascript_fallback': self.config.enable_javascript_fallback,
                        'enhanced_patterns': True
                    }
                }
                
                logger.info(f"üéØ {company_name}: {len(processed_executives)} executives in {processing_time:.2f}s")
                return result
                
            except Exception as e:
                logger.error(f"‚ùå Error processing {company_name}: {e}")
                return {
                    'company_name': company_name,
                    'executives_found': 0,
                    'executives': [],
                    'error': str(e),
                    'processing_time': time.time() - start_time
                }
    
    async def _discover_relevant_pages(self, base_url: str) -> List[Tuple[str, str]]:
        """Enhanced page discovery with intelligent prioritization"""
        if not base_url:
            return []
        
        relevant_pages = [(base_url, 'home')]
        
        # Enhanced page discovery patterns
        page_patterns = {
            'about': ['about', 'about-us', 'about_us', 'aboutus', 'our-story', 'company'],
            'team': ['team', 'staff', 'people', 'our-team', 'our_team', 'leadership', 'management'],
            'contact': ['contact', 'contact-us', 'contact_us', 'contactus', 'get-in-touch', 'reach-us'],
            'services': ['services', 'what-we-do', 'solutions', 'offerings'],
            'bio': ['bio', 'biography', 'profile', 'profiles', 'meet-the-team']
        }
        
        try:
            # Fetch home page to look for links
            soup = await self.session.fetch_content(base_url)
            if soup:
                # Find navigation links
                nav_links = soup.find_all('a', href=True)
                
                for link in nav_links:
                    href = link.get('href', '').lower()
                    link_text = link.get_text(strip=True).lower()
                    
                    # Convert relative URLs to absolute
                    if href.startswith('/'):
                        full_url = urljoin(base_url, href)
                    elif href.startswith('http'):
                        full_url = href
                    else:
                        continue
                    
                    # Check if this is a relevant page
                    for page_type, patterns in page_patterns.items():
                        if any(pattern in href or pattern in link_text for pattern in patterns):
                            if full_url not in [url for url, _ in relevant_pages]:
                                relevant_pages.append((full_url, page_type))
                                break
                
                # Limit pages for performance
                relevant_pages = relevant_pages[:self.config.max_pages_per_company]
                
        except Exception as e:
            logger.warning(f"Error discovering pages for {base_url}: {e}")
        
        return relevant_pages
    
    def _post_process_executives(self, executives: List[Dict], company_name: str) -> List[Dict]:
        """Enhanced post-processing with semantic validation"""
        if not executives:
            return []
        
        # Remove duplicates with enhanced logic
        unique_executives = []
        seen_names = set()
        
        for exec in executives:
            name_key = exec['name'].lower().strip()
            if name_key not in seen_names:
                seen_names.add(name_key)
                
                # Enhanced validation if enabled
                if self.config.enable_semantic_validation:
                    is_valid, confidence, reason = self.validator.is_valid_executive_name(exec['name'])
                    if is_valid and confidence >= self.config.min_confidence_score:
                        exec['final_confidence'] = confidence
                        exec['final_validation'] = reason
                        unique_executives.append(exec)
                else:
                    unique_executives.append(exec)
        
        # Sort by confidence score
        unique_executives.sort(key=lambda x: x.get('confidence_score', 0), reverse=True)
        
        return unique_executives
    
    def _calculate_enhanced_quality_metrics(self, executives: List[Dict], contacts: Dict, page_analyses: List[Dict]) -> Dict:
        """Calculate enhanced quality metrics with content intelligence"""
        if not executives:
            return {
                'quality_score': 0.0,
                'confidence_distribution': {'high': 0, 'medium': 0, 'low': 0},
                'contact_completeness': 0.0,
                'content_intelligence_score': 0.0
            }
        
        # Confidence distribution
        high_conf = sum(1 for e in executives if e.get('confidence_score', 0) >= 0.8)
        medium_conf = sum(1 for e in executives if 0.6 <= e.get('confidence_score', 0) < 0.8)
        low_conf = sum(1 for e in executives if e.get('confidence_score', 0) < 0.6)
        
        # Contact completeness
        total_contacts = len(contacts['emails']) + len(contacts['phones'])
        contact_completeness = min(total_contacts / (len(executives) * 2), 1.0) if executives else 0.0
        
        # Content intelligence score (based on page analyses)
        avg_context_score = sum(analysis.get('context_score', 0) for analysis in page_analyses) / len(page_analyses) if page_analyses else 0.0
        
        # Overall quality score
        quality_score = (
            (high_conf * 1.0 + medium_conf * 0.7 + low_conf * 0.4) / len(executives) * 0.4 +
            contact_completeness * 0.3 +
            avg_context_score * 0.3
        )
        
        return {
            'quality_score': quality_score,
            'confidence_distribution': {
                'high': high_conf,
                'medium': medium_conf,
                'low': low_conf
            },
            'contact_completeness': contact_completeness,
            'content_intelligence_score': avg_context_score,
            'page_analysis_count': len(page_analyses)
        }
    
    def cleanup(self):
        """Clean up resources"""
        self.session.cleanup()

async def main():
    """Main execution function for Phase 7A Content Intelligence Pipeline"""
    # Phase 7A Configuration
    config = Phase7AConfig(
        max_concurrent_companies=2,  # Conservative for quality
        max_pages_per_company=8,     # Balanced coverage
        enable_javascript_fallback=True,
        enable_context_analysis=True,
        enable_semantic_validation=True,
        min_confidence_score=0.7
    )
    
    # Test companies (enhanced set for better testing)
    test_companies = [
        {'name': 'Celm Engineering', 'website': 'https://celmeng.co.uk/'},
        {'name': 'MS Heating & Plumbing', 'website': 'https://msheatingandplumbing.co.uk/'},
        {'name': 'Bradley Mechanical', 'website': 'https://bradleymechanical.com/'},
        {'name': 'A-Advantage Heating', 'website': 'https://a-advantage.com/'},
        {'name': 'Complete Heating Solutions', 'website': 'https://completeheating.co.uk/'}
    ]
    
    # Initialize pipeline
    pipeline = Phase7AContentIntelligencePipeline(config)
    
    try:
        # Run Phase 7A content intelligence discovery
        results = await pipeline.discover_executives(test_companies)
        
        # Save results
        timestamp = int(time.time())
        results_file = f'phase7a_content_intelligence_results_{timestamp}.json'
        
        with open(results_file, 'w') as f:
            json.dump(results, f, indent=2, default=str)
        
        logger.info(f"üíæ Phase 7A content intelligence results saved to {results_file}")
        
        # Print summary
        summary = results['phase7a_summary']
        logger.info(f"")
        logger.info(f"=" * 70)
        logger.info(f"üéØ PHASE 7A CONTENT INTELLIGENCE RESULTS SUMMARY")
        logger.info(f"=" * 70)
        logger.info(f"Companies Processed: {summary['total_companies_processed']}")
        logger.info(f"Successful Companies: {summary['successful_companies']}")
        logger.info(f"Executives Found: {summary['total_executives_found']}")
        logger.info(f"Success Rate: {summary['success_rate']*100:.1f}%")
        logger.info(f"Processing Time: {summary['total_processing_time']:.1f}s")
        logger.info(f"Companies/Hour: {summary['companies_per_hour']:.1f}")
        logger.info(f"Avg Executives/Company: {results['performance_metrics']['average_executives_per_company']:.1f}")
        logger.info(f"=" * 70)
        logger.info(f"üìÑ Results saved to: {results_file}")
        logger.info(f"=" * 70)
        
        # Show top discoveries
        valid_results = [r for r in results['detailed_results'] if r.get('executives_found', 0) > 0]
        if valid_results:
            logger.info(f"üèÜ TOP DISCOVERIES:")
            for result in sorted(valid_results, key=lambda x: x['executives_found'], reverse=True)[:3]:
                logger.info(f"  ‚Ä¢ {result['company_name']}: {result['executives_found']} executives")
        
    finally:
        pipeline.cleanup()

if __name__ == "__main__":
    asyncio.run(main()) 